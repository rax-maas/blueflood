# Blueflood Grinder Integration
##Intro
Grinder is a distributed load testing tool described [here](http://grinder.sourceforge.net/g3/getting-started.html)

This code defines implementations of grinder worker threads meant to repeatedly invoke the required number of BF api calls during each "reporting interval"

It also includes the infrastructure to divide the total work described in the grinder properties file across all the workers in the distributed system.

##Architecture
The BF grinder system is designed to take a single properties file that lists the total amount
of requests to be generated by the system, and divide the total load across the number of
servers aka "agents" that are handling the load.  Each agent subdivides it's load across each of
it's worker threads.   Thus much of the initialization process is spent dividing up all the work.

The TestRunner class in contrib/grinder/scripts/grinder.py is the interface to the 
grinder itself and is how the grinder communicates with the BF grinder system.  The
TestRunner.__call__() method is invoked as many times as possible on each thread while
the grinder is running.

The BF grinder code creates different types of threads to handle the different kinds of requests
to be generated.  Currently the thread types are:

Ingest - create requests for the  "/ingest/multi" endpoint
Query - does single/multi plot and search requests
AnnotationsIngest - ingests to the "events" endpoint

The contrib/grinder/scripts/utils.py file contains the ThreadManager class which decides type of threads to create on each agent.  It also contains the AbstractThread class which is the parent of all the
thread types.

The basic layout of each of the threads is that each gets a list of metrics to process
during a "reporting interval", which is typically one minute.  Each thread attempts
to process just the metrics in that list during that reporting interval.  When it
finishes the list it goes to sleep until the start of the next reporting interval.
Each time the grinder's TestRunner.__call__ method is invoked, it calls the
threads make_request() method, which processes a single metric off the list of metrics
and returns the result to the grinder so that it can update it's statistics.

The list of metrics is constant for each agent and each thread and so
the same metrics get processed again each time a new reporting
interval starts.  At init time, the total list for an agent is divided
into "slices" and each time the grinder invokes the threads'
make_request() method, the thread processes another metric in it's
"slice".  If the make_request call is invoked after all the metrics in
the slice have been handled, the thread just goes to sleep.


##Installing
```bash
cd /tmp
wget https://sourceforge.net/projects/grinder/files/The%20Grinder%203/3.11/grinder-3.11-binary.zip/download
wget http://opensource.xhaus.com/attachments/download/3/jyson-1.0.2.zip
cd $BLUEFLOOD_INSTALL/contrib/grinder
mkdir resources
cd resources
unzip /tmp/download
unzip /tmp/jyson-1.0.2.zip
```

Note this needs to be run on each node in the cluster, as well as the console.

##Starting the console
The GUI is started like so:
```bash
cd $BLUEFLOOD_INSTALL/contrib/grinder
java -cp  resources/grinder-3.11/lib/grinder.jar:resources/jyson-1.0.2/lib/jyson-1.0.2.jar net.grinder.Console
```
The console can be run headless, like so:
```bash
java -cp  resources/grinder-3.11/lib/grinder.jar:resources/jyson-1.0.2/lib/jyson-1.0.2.jar net.grinder.Console -headless
```

and you interact with a rest api like so:
```bash
curl -X POST http://localhost:6373/agents/stop-workers
curl -X POST http://localhost:6373/agents/start-workers
```
The graphical console gives some useful status info, so you may prefer using that.


##Starting the agents
Each agent is started like so:
```bash
java -cp  resources/grinder-3.11/lib/grinder.jar:resources/jyson-1.0.2/lib/jyson-1.0.2.jar net.grinder.Grinder $GRINDER_PROPERTIES_FILE
```
There are currently some example properties files here:
```bash
$BLUEFLOOD_INSTALL/contrib/grinder/properties/
```

grinder.properties runs the unit tests
grinder.properties.local has some configs for running on your localhost
grinder.properties.staging runs the staging configuration and is meant to be run on two nodes

##Coverage tool
The coverage tool invocation is hardcoded into the tests.  (It's the only way I could get it
to work with Jython.)  To generate the corresponding html output, run:
```bash
coverage html -d /tmp/grinder
```
